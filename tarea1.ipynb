{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA6202: Laboratorio de Ciencia de Datos\n",
    "\n",
    "**Profesor: Nicolás Caro**\\\n",
    "**Auxiliar: Rodrigo Lara M**\n",
    "\n",
    "**17/05/2020 - Tarea 1**\n",
    "\n",
    "\n",
    "**Integrantes del grupo**: Pablo Araya Z., Fabian Badilla M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introducción\n",
    "La siguiente evaluación corresponde a la primera tarea del curso de laboratorio de ciencia de datos. A modo de contexto, se trabajará con un conjunto de datos, en el cual se busca una estimación del precio por metro cuadrado para viviendas en la ciudad de Bogotá, se busca además de una estimación de incertidumbre en predicción.\\\n",
    "Para ello, se proporcionan datos recolectados por _web-scrapping_ y una serie de estadísticas que caracterizan determinadas zonas geográficas de interés, llamadas Unidades de Planeación Zonal (UPZ).\n",
    "\n",
    "Las condiciones de entrega requeridas son:\n",
    "\n",
    "* La extensión máxima de el informe es de 6 planas a las que puede añadir 2 para demostraciones.\n",
    "* Debe adjuntar un repositorio `git` donde se incluya todo su código.\n",
    "* A lo menos 1 `commit` por cada pregunta de la tarea\n",
    "* Por lo menos 1 `merge` a través de su trabajo.\n",
    "* Incluya un documento `jupyter notebook` llamado `tarea1.ipynb` en el cual se exponga todo el procedimiento realizado.\n",
    "* Por último es necesario también entregar un archivo _pickle_ denominado `modelo.pk` que contenga el último modelo de regresión entrenado.\n",
    "\n",
    "Tenga en mente que su informe será revisado por un equipo técnico que debe entender a cabalidad su metodología, ser capaz de replicarlo y evaluarlo a partir de la lectura de este."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import warnings\n",
    "from IPython.display import clear_output,display_html\n",
    "import missingno as msno\n",
    "from sklearn.base import RegressorMixin, BaseEstimator\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1. Carga y limpieza de datos\n",
    "En la presente sección se realizan los pasos de carga y limpieza de datos que permitirán realizar las secciones\n",
    "posteriores con un `DataFrame` consolidado que presente los tipos de datos adecuados para la información contenida\n",
    "en sus columnas.\\\n",
    "Incluya en el reporte todas las decisiones que llevó en esta secci\u0013on, adem\u0013as de reportar y discutir acerca de\n",
    "los aspectos específicos señalados en cada pregunta.\n",
    "\n",
    "1. Los datos recolectados en la carpeta `data/raw` están divididos en carpetas con el formato `wNN` donde `NN` corresponde a la semana del año en la que estos fueron consultados. Cargue los datos en un solo `DataFrame` y elimine las filas duplicadas. Además genere una variable categórica en la que se indique si la observación correspondiente proviene de un archivo que en su nombre contiene `'furnished'`\\\n",
    "(ejemplo: metrocuadrado_furnished_wNN.csv).\\\n",
    "**Hint:** para lo último puede ser útil estudiar los argumentos del\n",
    "método `pd.merge`. Reporte si existen observaciones de archivos con texto `'furnished'` que no estén contenidos en archivos con texto `'all'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 2099 Observaciones de furnished que no estan contenidas en all\n"
     ]
    }
   ],
   "source": [
    "''' Se juntan los dfs de los csv con nombre \"all\" '''\n",
    "df_s = []\n",
    "for i in range(13,18):\n",
    "    w = \"w\"+str(i)\n",
    "    direccion = 'data/raw/'+w+'/metrocuadrado_all_'+w+'.csv'\n",
    "    df = pd.read_csv(direccion)\n",
    "    df_s.append(df)\n",
    "df_all = pd.concat(df_s,axis=0,ignore_index = True)\n",
    "\n",
    "'''Creamos la variable categorica'''\n",
    "df_all['Archivo'] = 'all'\n",
    "\n",
    "''' Eliminamos los datos duplicados'''\n",
    "df_all.drop_duplicates(inplace=True,ignore_index=True)\n",
    "\n",
    "''' Se juntan los dfs de los csv con nombre \"furnished\" '''\n",
    "df_s = []\n",
    "for i in range(13,18):\n",
    "    w = \"w\"+str(i)\n",
    "    direccion = 'data/raw/'+w+'/metrocuadrado_furnished_'+w+'.csv'\n",
    "    df = pd.read_csv(direccion)\n",
    "    df_s.append(df)\n",
    "df_fur = pd.concat(df_s,axis=0,ignore_index = True)\n",
    "\n",
    "'''Creamos la variable categorica'''\n",
    "df_fur['Archivo'] = 'furnished'\n",
    "\n",
    "''' Eliminamos los datos duplicados'''\n",
    "df_fur.drop_duplicates(inplace=True,ignore_index=True)\n",
    "\n",
    "''' Juntamos los dos dfs'''\n",
    "df = pd.concat([df_all,df_fur],axis=0,ignore_index = True)\n",
    "\n",
    "''' Eliminamos posibles datos de furnished que esten en all'''\n",
    "df.drop_duplicates(['property_type|rent_type|location', 'price', 'n_rooms', 'n_bath','surface', 'details',\n",
    "                    'url', 'metrocuadrado_index'],inplace=True,ignore_index=True)\n",
    "\n",
    "print('Hay',sum(df['Archivo']=='furnished'),'Observaciones de furnished que no estan contenidas en all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Limpie las columnas:\n",
    "    1. Precio y de área del inmueble, número de habitaciones y número de baños.\n",
    "    2. De `'property type|rent type|location'`. Debe obtener 3 columnas en las que se detalle el tipo de inmueble (casa o apartamento), el tipo de la oferta (arriendo o arriendo y venta) además de el barrio en el cual se ubica el inmueble (texto en mayúsculas para la mayoría de los casos) Llame a esta última columna `'location'`.\\\n",
    "    __Hint:__ Para ello pueden ser útiles los métodos `str.split` y `str.strip` de la clase `pd.Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "property_type|rent_type|location     object\n",
       "price                               float64\n",
       "n_rooms                              object\n",
       "n_bath                               object\n",
       "surface                             float64\n",
       "details                              object\n",
       "url                                  object\n",
       "metrocuadrado_index                 float64\n",
       "Archivo                              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Quitamos los nan '''\n",
    "df = df.dropna(axis = 0)#,subset=['price','n_rooms','n_bath','surface'])\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "''' Limpiamos los datos de price y surface '''\n",
    "df['price'] = [x.replace('$','') for x in df['price']]\n",
    "df['price'] = [x.replace('.','') for x in df['price']]\n",
    "df['price'] = df['price'].astype('float64')\n",
    "df['surface'] = [x.replace(' m2','') for x in df['surface']]\n",
    "df['surface'] = df['surface'].astype('float64')\n",
    "\n",
    "''' Cambiar el formato de los datos n_bath y n_rooms'''\n",
    "for i in range(len(df['n_bath'])):\n",
    "    if type(df['n_bath'][i]) == float:\n",
    "        df['n_bath'][i] = str(int(df['n_bath'][i]))\n",
    "        \n",
    "for i in range(len(df['n_rooms'])):\n",
    "    if type(df['n_rooms'][i]) == float:\n",
    "        df['n_rooms'][i] = str(int(df['n_rooms'][i]))\n",
    "            \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Separar la primera columna en las 3 columnas respectivas'''\n",
    "\n",
    "columna = 'property_type|rent_type|location'\n",
    "first_col = df[columna]\n",
    "first_col = pd.Series([x.replace(' ',',', 1) for x in first_col])\n",
    "first_col = first_col.str.split(pat=',',expand =True)\n",
    "first_col.columns = columna.split('|')\n",
    "first_col['rent_type'] =  [x.replace('en ','') for x in first_col['rent_type']]\n",
    "df = pd.concat([first_col,df.iloc[:,1:]],axis=1)\n",
    "df['location'] = [x.replace(' ','', 1) for x in df['location']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Adjunte las siguientes columnas:\n",
    "    1. Genere una variable que represente el precio por metro cuadrado.\n",
    "    2. Obtenga el número de garajes procesando la columna `'url'` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Se genera la nueva columna del precio por metro cuadrado'''\n",
    "df['Precio_m2'] = df['price']/df['surface']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Generar la columna del numero de garajes'''\n",
    "num_gar = []\n",
    "for x in df['url']:\n",
    "    if '-garajes' not in x:\n",
    "        num_gar.append('0')\n",
    "    else:\n",
    "        num_gar.append(x[x.find('banos-')+6:x.find('-garajes')])\n",
    "df['Numero_Garajes'] = num_gar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Clasifique las observaciones por tipo de producto de acuerdo a los criterios de la Tabla\n",
    "    1. Hint: puede ser útil el método `query` de `pd.DataFrame`\n",
    "\n",
    "|Tipo de Producto| Tipo de Inmueble| área min | área max |\n",
    "|----------------|-----------------|----------|----------|\n",
    "|        1       |       casa      | 80       |    <120  |\n",
    "|        2       |       casa      | 120      |    <180  |\n",
    "|        3       |       casa      | 180      |    <240  |\n",
    "|        4       |       casa      | 240      |    <360  |\n",
    "|        5       |       casa      | 360      |    460   |\n",
    "|        6       |   apartamento   | 40       |    <60   |\n",
    "|        7       |   apartamento   | 60       |    <80   |\n",
    "|        8       |   apartamento   | 80       |    120   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Generar el tipo de producto segun la condicion'''\n",
    "\n",
    "condiciones = ['80<=surface<120 and property_type==\"Casa\"','120<=surface<180 and property_type==\"Casa\"',\n",
    "              '180<=surface<240 and property_type==\"Casa\"','240<=surface<360 and property_type==\"Casa\"',\n",
    "              '360<=surface<=460 and property_type==\"Casa\"','40<=surface<60 and property_type==\"Apartamento\"',\n",
    "              '60<=surface<80 and property_type==\"Apartamento\"','80<=surface<=120 and property_type==\"Apartamento\"']\n",
    "df['tipo_producto'] = np.zeros(len(df))\n",
    "for i in range(1,9):\n",
    "    ind = df.query(condiciones[i-1]).index\n",
    "    df.loc[ind,'tipo_producto']=int(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_indice = df[df['tipo_producto']==0].index\n",
    "buen_indice = [x for x in df.index if x not in mal_indice]\n",
    "df = df.loc[buen_indice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. A partir de la columna `'barrio'`, haga una fusión con el archivo `data/assignacion upz/barrio-upz-asignacion.csv` para obtener así el código de la UPZ de cada inmueble. Reporte el numero de observaciones y de barrios a los que no se les puede adjuntar un código UPZ a partir de este archivo. Tenga en cuenta que aproximadamente 90% de los datos tiene información sobre UPZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N° Observaciones sin codigo: 1640\n",
      "% Observacions sin codigo: 89.346\n"
     ]
    }
   ],
   "source": [
    "'''Cargamos los datos'''\n",
    "barrio_upz =pd.read_csv('data/asignacion_upz/barrio-upz.csv')\n",
    "barrio_upz.drop(['UPlTipo','UPlNombre'], axis=1, inplace=True)\n",
    "\n",
    "''' Para realizar lo anterior hay que limpiar la columna location'''\n",
    "df['location'] =  [x.replace(' Bogotá D.C..','') for x in df['location']]\n",
    "df['location'] =df['location'].str.lower()\n",
    "\n",
    "''' Cambiamos el nombre de la columna para poder juntar los df '''\n",
    "barrio_upz.rename(columns={\"pro_location\": \"location\"},inplace = True)\n",
    "\n",
    "'''Juntamos los data frames'''\n",
    "df = pd.merge(df,barrio_upz,how ='left',on ='location')\n",
    "\n",
    "''' Reporte de a cuantos barrios no les pudo asignar codigo'''\n",
    "num_nan = df['UPlCodigo'].isna().sum()\n",
    "por_nan = 100-df['UPlCodigo'].isna().sum()*100/len(df)\n",
    "print('N° Observaciones sin codigo:',num_nan)\n",
    "print('% Observacions sin codigo:',por_nan.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. En la carpeta `data/estadisticas_upz` encontraría todos los archivos que debe fusionar con su `DataFrame`, a través del código de UPZ, para así enriquecer su conjunto de datos con estadísticas de población, socioeconómicas y de calidad de vida a nivel de UPZ. Una vez realizada la fusión, adjunte una nueva columna con la densidad de población por UPZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Cargamos los 3 csv'''\n",
    "esta = pd.read_csv('data/estadisticas_upz/estadisticas_poblacion.csv',index_col = 0)\n",
    "ind = pd.read_csv('data/estadisticas_upz/indice_inseguridad.csv',index_col = 0)\n",
    "porct = pd.read_csv('data/estadisticas_upz/porcentaje_areas_verdes.csv',index_col = 0)\n",
    "\n",
    "''' Cambiamos los datos para poder hacer el merge'''\n",
    "esta.rename(columns={\"upz\": \"UPlCodigo\"},inplace = True)\n",
    "porct.rename(columns={\"cod_upz\": \"UPlCodigo\"},inplace = True)\n",
    "porct['UPlCodigo']= porct['UPlCodigo'].apply(int)\n",
    "porct['UPlCodigo'] = ['UPZ'+str(x) for x in porct['UPlCodigo']]\n",
    "\n",
    "''' Juntamos los dfs '''\n",
    "df = pd.merge(df,esta,how ='left',on ='UPlCodigo')\n",
    "df = pd.merge(df,ind,how ='left',on ='UPlCodigo')\n",
    "df = pd.merge(df,porct,how ='left',on ='UPlCodigo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Generar la densidad poblacional por UPZ'''\n",
    "df['Density'] = df['personas']/df['UPlArea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Se borran columnas repetidas o que no aportan info util'''\n",
    "df.drop(['nomupz','UPlNombre2','url','details','upz'], axis=1, inplace=True)\n",
    "df.rename(columns={\"upz\": \"UPlNombre\"},inplace = True)\n",
    "df.to_pickle('./data/data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Precio_m2','surface']:\n",
    "    q = df[col].quantile(0.98)\n",
    "    df=df[df[col] < q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2. EDA\n",
    "En la siguiente pregunta utilice el `DataFrame` obtenido a partir de los procedimientos anteriores. La idea central del presente ejercicio es analizar la base de datos que fue construida realizando un análisis exploratorio de datos (EDA) riguroso que permite obtener información útil para la parte final de tarea. Para cada pregunta y considerando que la variable de respuesta es el precio por metro cuadrado, discuta sus resultados y reporte algunos gráficos interesantes en su informe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Programe una función `estilo()` que aplica un estilo de gráficos por defecto diseñado por usted. Este estilo debe ser empleado en todos los gráficos que incluya en su informe.\\\n",
    "__Hint:__ Use métodos de la librería `seaborn` para ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Perfile las variables obtenidas en su `DataFrame`, agrúpelas y analícelas según su naturaleza. En función de su agrupación, grafíque las distribuciones dando un tratamiento adecuado a cada tipo de variable, discuta ciertos casos de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Estudie la presencia de datos faltantes en la base de datos. Observe como se distribuyen estos y establezca un mecanismo de pérdida de información basándose en los patrones observables del conjunto de datos. Busque agrupaciones de columnas que muestren un comportamiento sistemático y plantee sus reflexiones.\\\n",
    "Respalde con visualizaciones y cuantifique estadísticamente los patrones observados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Recategoríce la variable código de UPZ de forma que quede distribuida entre 3 a 5 grupos, evalúe la significancia estadística de esta nueva agrupación en comparación a la variable de respuesta. Comente sus resultados e interprételos.\\\n",
    "__Hint:__ Puede probar con técnicas de clustering (como k-means) sobre una agrupación de UPZ y validar estadísticamente si las nuevas categorías afectan la variable de respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Cuantifique estadísticamente las relaciones entre una selección de al menos 10 variables de interés y la columna de respuesta, examine también las relaciones entre las variables de su selección. Utilice las herramientas de análisis estadístico que considere pertinentes, comente brevemente sus hallazgos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. En base a las herramientas del curso realice un análisis que permita detectar observaciones anómalas en la base de datos, justifique sus resultados, evalúe como se distribuyen los valores anómalos respecto a las variables UPZ y tipo de producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. En función del análisis realizado a lo largo de esta pregunta, proponga una selección de variables que permita estimar la variable respuesta por medio de un modelo de regresión, discuta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3. Regresión lineal bayesiana\n",
    "Una vez analizado el conjunto de datos, se procede a modelar las relaciones por medio de alguna herramienta matemática. En este contexto y con las herramientas computacionales entregadas por el curso, se desarrolla un modelo de regresión lineal bayesiana, tratando el problema de modelación desde el punto vista teórico hasta su implementación e interpretación de resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementación\n",
    "1. Implemente la clase `RegresionBayesianaEmpirica` que herede de `BaseEstimator` y de `RegressorMixin` del módulo `sklearn.base` en la cual se implementa la heurística enunciada en la sección anterior para aproximar los hiperparámetros óptimos $\\alpha$ y $\\beta$. Esta clase sólo debe usar objetos de la librería `NumPy` y debe incluir al menos los siguientes métodos:\n",
    "    * `__init__(self, alpha_0, beta_0, tol=1e-5, maxiter=200)`: sus argumentos son auto explicativos.\n",
    "    * `get_posteriori(self, X, y, alpha, beta)`: que reciba la matriz de observaciones (`X`), el vector de etiquetas (`y`) y los hiperparámetros $\\alpha$ y $\\beta$. Este debe retornar los objetos necesarios para interactuar con los demás métodos.\n",
    "    * `fit(self, X, y)`: que reciba la matriz de observaciones (`X`), el vector de etiquetas (`y`) e implemente el esquema de aproximación mencionado. Este método debe guardar como atributos del objeto los parámetros óptimos obtenidos, además de reportar en pantalla indicadores del proceso iterativo (incluya al menos el número de iteraciones).\n",
    "    * `predict(self, X_ , return_std=False)`: que reciba una matriz de observaciones (`X_`). Debe retornar la tupla (`y_` , `y_std`) con el vector de medias y el de desviaciones estándar (cuando `return std=True`) asociadas a las observaciones en `X`. Para esto, observe que el proceso de predicción corresponde a asignar la media posterior predictiva del modelo a nuevos puntos.\n",
    "    \n",
    "   Note que todo desarrollo sólo necesita de un modelo lineal en los parámetros $w$. Es decir, es posible reemplazar $X$ por una transformación (posiblemente no lineal) $\\Phi(X)$, manteniendo la misma estructura distrubucional tanto en predicción como en obtención de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,RegressorMixin\n",
    "\n",
    "class RegresionBayesianaEmpirica(BaseEstimator,RegressorMixin):\n",
    "    def __init__(self,alpha_0,beta_0,tol=1e-5,maxiter=200):\n",
    "        ''' Parametros del Modelo '''\n",
    "        self.alpha_0 = alpha_0\n",
    "        self.beta_0 = beta_0\n",
    "        self.tol = tol\n",
    "        self.maxiter = maxiter\n",
    "    \n",
    "    \n",
    "    def get_posteriori(self,X,y,alpha,beta):\n",
    "        ''' Parametros de la distribucion '''\n",
    "        N,d = X.shape\n",
    "        SN_inv = alpha*np.identity(d) + beta*(X.T@X)\n",
    "        SN = np.linalg.inv(SN_inv)\n",
    "        mN = beta*(SN@X.T@y)\n",
    "        return mN,SN\n",
    "              \n",
    "    def fit(self,X,y):\n",
    "        ''' Parametros iniciales '''\n",
    "        N,d = X.shape\n",
    "        iteraciones = self.maxiter\n",
    "        tolerancia = self.tol\n",
    "        i=1\n",
    "        while i<iteraciones+1:\n",
    "            print('Iteracion '+str(i))\n",
    "            \n",
    "            ''' Paramatros iniciales de la iteracion '''\n",
    "            alfa_inicial = self.alpha_0\n",
    "            beta_inicial = self.beta_0\n",
    "            mN,SN = self.get_posteriori(X,y,alfa_inicial,beta_inicial)\n",
    "            \n",
    "            ''' Calculamos los valores propios y los vectores propios'''\n",
    "            lamb,vect = np.linalg.eig(beta_inicial*(X.T@X))\n",
    "            lamb = np.real_if_close(lamb,tol = 1)\n",
    "            \n",
    "            gamma =(lamb/(alfa_inicial + lamb)).sum()\n",
    "            \n",
    "            ''' Posibles Nuevos valores'''\n",
    "            alpha = gamma/(mN.T@mN)\n",
    "            beta = (N-gamma)/(((y-mN.T@X.T)**2).sum())\n",
    "            \n",
    "            '''Verificar si son casi iguales '''\n",
    "            error_alpha = abs(alpha - alfa_inicial)\n",
    "            error_beta = abs(beta - beta_inicial )\n",
    "            print('alpha: ',alpha,'    beta: ',beta)\n",
    "            if i%10 ==0:\n",
    "                print('Error Alpha: ',error_alpha,'   Error Beta: ',error_beta)\n",
    "            if error_alpha <tolerancia and error_beta < tolerancia:\n",
    "                ''' Guardamos los parametros encontrados '''\n",
    "                self.alpha_0 = alpha\n",
    "                self.beta_0 = beta\n",
    "                break\n",
    "            else:\n",
    "                self.alpha_0 = alpha\n",
    "                self.beta_0 = beta\n",
    "                i = i+1\n",
    "        self.mN,self.SN = self.get_posteriori(X,y,alpha,beta)\n",
    "        ''' Reporte del Resultado Final'''\n",
    "        print('\\n')\n",
    "        print('Resultados Finales')\n",
    "        print('alpha final = ',alpha)\n",
    "        print('beta final = ',beta)\n",
    "            \n",
    "            \n",
    "    def predict(self,X_,return_std=False):\n",
    "        ''' Generamos el vector de medias y el de desviaciones estándar '''\n",
    "        y_ = X_ @ self.mN\n",
    "        y_std = 1/self.beta_0 + np.diag((X_ @ self.SN)@X_.T)\n",
    "        if return_std == False:\n",
    "            return y_\n",
    "        if return_std == True:\n",
    "            return (y_,y_std)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las siguientes preguntas se construye un modelo de regresion en el cual debe incluir todas las variables disponibles en el conjunto de datos, reemplazando la variable de UPZ por la recategorizacion antes propuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Construya un fujo de transformaciones sobre el conjunto datos. Por medio de la clase `Pipeline` debería:\n",
    "    * Utilizar `StandardScaler`,`MinMaxScaler`, y `OneHotEncoder` donde corresponda.\n",
    "    * Utilizar el objeto `PolynomialFeatures` para generar características polinomiales, en este apartado, se recomienda utilizar características de grado 3 sólo en las variables numéricas y luego concatenar con las codificaciones categóricas.\n",
    "    * Generar una composición de transformaciones por medio de `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Separamos los datos en X e y'''\n",
    "df_y = df_s.iloc[:,:]['Precio_m2']#.to_numpy()\n",
    "df_X = df_s.drop(['Precio_m2','price','surface','UPlTipo','UPlCodigo','personas','UPlArea'],axis=1).iloc[:,:]#.to_numpy()\n",
    "\n",
    "''' Filtramos las filas por tipo de datos'''\n",
    "num_cols = df_X.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = df_X.select_dtypes(include=['object']).columns.drop('property_type')\n",
    "num_perc_cols = num_cols.drop('Density','metrocuadrado_index') \n",
    "num_cols = ['Density','metrocuadrado_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Importamos lo necesario para transformar los datos '''\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,OneHotEncoder,PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "''' Pipeline para las variables categoricas '''\n",
    "cat_pipe = Pipeline([('ordinal', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "''' Pipeline para las variables numericas '''\n",
    "num_pipe1 = Pipeline([('scaler', MinMaxScaler()), ('poly', PolynomialFeatures(degree=3))])\n",
    "num_pipe2 = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(degree=3))])\n",
    "\n",
    "''' Aplicamos las transformaciones a las columnas correspondientes '''\n",
    "prep = ColumnTransformer(transformers=[('numeric1', num_pipe1, num_perc_cols),('numeric2', num_pipe2, num_cols), \n",
    "                                       ('categoric', cat_pipe, cat_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline([('ordinal', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "num_pipe = Pipeline([('scaler',StandardScaler()),('poly',PolynomialFeatures(degree = 3))])\n",
    "\n",
    "prep = ColumnTransformer(transformers = [('num',num_pipe,num_cols),('cat',cat_pipe,cat_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Expanda el `Pipeline` anterior agregando el modelo representado en la clase `RegresionBayesianaEmpirica`. Genere una separación de entrenamiento y test por medio de `train_test_split` del módulo `model_selection` donde el 20% de los datos sea de test. Entrene su modelo utilizando el método .fit como parámetros dentro del flujo creado por medio de Pipeline, evalúe su modelo por medio de la raíz del promedio de errores cuadráticos (_Root Mean Square Error - RMSE_. en inglés) en el conjunto de test, incluya el estadístico $R^2$.\\\n",
    "__Hint__: pruebe con valores para `alpha_0` y `beta_0` entre $10^{-10}$ y $10^{-5}$. Compruebe que dichos dichos hiperparametros son adecuados, al obtener un $R^2$ cercano a $0.7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def run_pipeline(df_X,df_Y,pipeline):\n",
    "    ''' Separamos el data set en datos de entrenamiento y datos de testeo'''\n",
    "    X_train,X_test,y_train,y_test = train_test_split(df_X,df_y,test_size=0.2,random_state = 31)\n",
    "    \n",
    "    ''' Fiteamos los datos a traves del pipeline'''\n",
    "    pipeline.fit(X_train,y_train)\n",
    "    \n",
    "    ''' Predecimos el valor de los valores de testeo '''\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    ''' Valores que sirven para ver que tan buena fue la prediccion '''\n",
    "    R2 = pipeline.score(X_test,y_test)\n",
    "    RMSE = np.sqrt(np.mean((y_test-y_pred)**2))\n",
    "    return pipeline,R2,RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Utilice su selección de variables, transfórmelas adaptando el esquema de preprocesamiento anterior, separe en conjuntos de entrenamiento y test manteniendo el 20% de proporción y evalúe los resultados del modelo `RegresionBayesianaEmpirica` incluyendo el estadístico $R^2$. Discuta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Inicial</th>\n",
       "      <td>2.000000e-10</td>\n",
       "      <td>2.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final</th>\n",
       "      <td>1.279651e-07</td>\n",
       "      <td>2.067244e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Errores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>0.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>7302.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Se generan las grillas para alpha y beta iniciales '''\n",
    "grilla_alpha = np.linspace(2e-10,1e-5,num=1,endpoint=False)\n",
    "grilla_beta = np.linspace(2e-10,1e-5,num=1,endpoint=False)\n",
    "\n",
    "''' Listas con los valores deseados'''\n",
    "valores = []\n",
    "valores_R2 = []\n",
    "valores_pipe = []\n",
    "valores_RMSE = []\n",
    "\n",
    "''' Iteramos con los alphas y betas'''\n",
    "for alpha in grilla_alpha:\n",
    "    for beta in grilla_beta:\n",
    "        print('Alpha Inicial: ',alpha,'   Beta Inicial: ',beta)\n",
    "        Pipe_Transformaciones = Pipeline([('prep',prep),('classifier',RegresionBayesianaEmpirica(alpha,beta,tol=1e-15))])\n",
    "        Pipe,R2, RMS2 = run_pipeline(df_X,df_y,Pipe_Transformaciones)\n",
    "        \n",
    "        ''' Guardamos los parametros '''\n",
    "        valores_R2.append(R2)\n",
    "        valores.append([alpha,beta])\n",
    "        valores_pipe.append(Pipe)\n",
    "        valores_RMSE.append(RMS2)\n",
    "        \n",
    "        ''' Limpiamos los output obtenidos '''\n",
    "        clear_output(wait=True)\n",
    "\n",
    "''' Los parametros obtenidos en el alpha y beta optimos '''\n",
    "optimo = valores[valores_R2.index(max(valores_R2))]\n",
    "Pipe_opt = valores_pipe[valores_R2.index(max(valores_R2))]\n",
    "R2 = valores_R2[valores_R2.index(max(valores_R2))]\n",
    "RMSE = valores_RMSE[valores_R2.index(max(valores_R2))]\n",
    "alpha = Pipe_opt.get_params(deep=False)['steps'][1][1].get_params()['alpha_0']\n",
    "beta = Pipe_opt.get_params(deep=False)['steps'][1][1].get_params()['beta_0']\n",
    "\n",
    "''' Funcion para mostrar dos tablas a la vez '''\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "\n",
    "\n",
    "''' Mostramos la informacion importante '''\n",
    "alpha_beta = pd.DataFrame([[optimo[0],optimo[1]],[alpha,beta]],index = ['Inicial','Final'],columns = ['Alpha',' Beta'])\n",
    "Parametros = pd.DataFrame([R2,RMSE],index = ['R2',' RMSE'],columns = ['Errores'])\n",
    "display_side_by_side(alpha_beta,Parametros.round(3))\n",
    "# print('Resultado Optimo')\n",
    "# print('alpha inicial: ',optimo[0],'     beta inicial: ',optimo[1])\n",
    "# print('alpha final: ',alpha,'     beta final: ',beta)\n",
    "# print('RMSE: ',RMSE)\n",
    "# print('R^2: ',R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finalmente, compare los resultados de su selección de variables con los obtenidos por un pipeline donde el estimador sea uno de la clase `BayesianRidge` del módulo `sklearn.linear_model`. Discuta la diferencia de este módulo con respecto al desarrollado por ustedes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-becdb1670cb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename = \"modelo.pkl\"\n",
    "pickle.dump(la_pipe,open(filename,'wb'))\n",
    "\n",
    "loaded_model = pickle.load(open(filename,'rb'))\n",
    "result = loaded_model.score(X_test,y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "ridge_pipe = Pipeline([('prep',prep),('classifier',BayesianRidge())])\n",
    "cosa,R2,RMS2 = run_pipeline(df_X,df_y,ridge_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6540374032527367"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df_X,df_y,test_size=0.2,random_state = 31)\n",
    "cosa.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
